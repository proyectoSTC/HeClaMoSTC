{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5fWnNsFGCr0"
      },
      "source": [
        "# Herramienta de ClasificaciÃ³n de Movimientos para STC\n",
        "\n",
        "**Pipeline optimizado:**\n",
        "1. Carga seÃ±ales completas\n",
        "2. Filtrado Butterworth + Notch\n",
        "3. SeparaciÃ³n por repeticiones (Train/ Val rep[2] / Test rep[5])\n",
        "4. NormalizaciÃ³n Z-score\n",
        "5. Ventanas (500ms, 25% overlap)\n",
        "6. ExtracciÃ³n features (ML) / Secuencias (DL)\n",
        "7. Diferentes balances de clases en train\n",
        "8. Entrenamiento y evaluaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDRCQIbt3Frg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7-a2Kgh9mvl"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“¦ SECCIÃ“N 1: INSTALACIÃ“N Y IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Instalar paquetes\n",
        "!pip install imbalanced-learn pywavelets -q\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "from scipy.io import loadmat\n",
        "import pywt\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "from imblearn.over_sampling import ADASYN, SMOTE\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, Model, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Definir funciÃ³n nombrada ANTES de construir modelos\n",
        "def attention_sum(x):\n",
        "    \"\"\"FunciÃ³n nombrada para reemplazar lambda en atenciÃ³n\"\"\"\n",
        "    return K.sum(x, axis=1)\n",
        "# Registrar como custom object\n",
        "keras.utils.get_custom_objects()['attention_sum'] = attention_sum\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"âœ… Imports completados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8s5ejb99qC8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# âš™ï¸ SECCIÃ“N 2: CONFIGURACIÃ“N\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    # Rutas\n",
        "    BASE_DIR = Path('/content/drive/MyDrive')\n",
        "    DATA_DIR = BASE_DIR / 'DB2_E1_only'\n",
        "    TRAIN_DIR = DATA_DIR / 'train'\n",
        "    SAVE_DIR = BASE_DIR / 'New_ML_DL_models_stc_optimized'\n",
        "\n",
        "    # ParÃ¡metros de seÃ±al OPTIMIZADOS\n",
        "    FS = 2000  # Hz\n",
        "    WINDOW_SIZE_MS = 500  # ðŸ”¥ CAMBIADO: 300 â†’ 500ms\n",
        "    WINDOW_SIZE = int(FS * WINDOW_SIZE_MS / 1000)\n",
        "    OVERLAP = 0.25  # ðŸ”¥ CAMBIADO: 0.5 â†’ 0.25\n",
        "    STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP))\n",
        "    N_CHANNELS = 12\n",
        "\n",
        "    # Filtrado\n",
        "    LOWCUT = 20\n",
        "    HIGHCUT = 450\n",
        "    NOTCH_FREQ = 50\n",
        "    NOTCH_Q = 30\n",
        "\n",
        "    # ClasificaciÃ³n binaria\n",
        "    RISK_MOVEMENTS = [13, 14, 15, 16]\n",
        "    SAFE_MOVEMENTS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 17]\n",
        "\n",
        "    # Repeticiones\n",
        "    TRAIN_REPS = [1, 3, 4, 6]\n",
        "    VAL_REPS = [2]\n",
        "    TEST_REPS = [5]\n",
        "\n",
        "    # DL params OPTIMIZADOS\n",
        "    BATCH_SIZE = 64  # ðŸ”¥ CAMBIADO: mÃ¡s grande\n",
        "    EPOCHS = 100\n",
        "    LEARNING_RATE = 0.0005\n",
        "    DROPOUT = 0.35  # ðŸ”¥ CAMBIADO: 0.4 â†’ 0.22\n",
        "    L2_REG = 0.001\n",
        "\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2EvpTh198tR"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸŽ¯ SECCIÃ“N 3: SELECCIÃ“N DE MODELOS Y SUJETOS\n",
        "# ============================================================================\n",
        "\n",
        "# ðŸ‘¤ CONFIGURACIÃ“N DE SUJETOS\n",
        "USE_ALL_SUBJECTS = True  # Cambia a False para usar lista especÃ­fica\n",
        "SELECTED_SUBJECTS = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]  # Lista de sujetos a usar (ignorada si USE_ALL_SUBJECTS=True)\n",
        "\n",
        "# ðŸ¤– CONFIGURACIÃ“N DE MODELOS\n",
        "SELECTED_MODELS = ['1', '2', '3', '4']  # Todos los modelos\n",
        "\n",
        "AVAILABLE_MODELS = {\n",
        "    '1': {'name': 'EnsembleSubspaceKNN', 'display': 'Ensemble KNN', 'type': 'ml'},\n",
        "    '2': {'name': 'RandomForest', 'display': 'RF', 'type': 'ml'},\n",
        "    '3': {'name': 'CNN_LSTM_Attention', 'display': 'CNN+LSTM+Attention', 'type': 'dl'},\n",
        "    '4': {'name': 'BiLSTM_Attention', 'display': 'BiLSTM+Attention', 'type': 'dl'},\n",
        "}\n",
        "\n",
        "ML_MODELS = [m for m in SELECTED_MODELS if AVAILABLE_MODELS[m]['type'] == 'ml']\n",
        "DL_MODELS = [m for m in SELECTED_MODELS if AVAILABLE_MODELS[m]['type'] == 'dl']\n",
        "NEED_FEATURES = len(ML_MODELS) > 0\n",
        "\n",
        "# ====== SELECCIÃ“N DE TÃ‰CNICAS DE BALANCEO ======\n",
        "# Para ML: 'none', 'adasyn', 'smote'\n",
        "# Para DL: 'none', 'augment_only', 'focal_loss', 'focal_loss+augment'\n",
        "\n",
        "ML_BALANCE_TECHNIQUE = 'adasyn'  # Cambiar aquÃ­\n",
        "DL_BALANCE_TECHNIQUE = 'augment_only'  # Cambiar aquÃ­\n",
        "\n",
        "# ValidaciÃ³n\n",
        "assert ML_BALANCE_TECHNIQUE in ['none', 'adasyn', 'smote'], \"TÃ©cnica ML invÃ¡lida\"\n",
        "assert DL_BALANCE_TECHNIQUE in ['none', 'augment_only', 'focal_loss', 'focal_loss+augment'], \"TÃ©cnica DL invÃ¡lida\"\n",
        "\n",
        "print(f\"ðŸ“Š ConfiguraciÃ³n:\")\n",
        "print(f\"   Sujetos: {'TODOS' if USE_ALL_SUBJECTS else SELECTED_SUBJECTS}\")\n",
        "print(f\"   Modelos ML: {ML_MODELS}\")\n",
        "print(f\"   Modelos DL: {DL_MODELS}\")\n",
        "print(f\"   TÃ©cnica balanceo ML: {ML_BALANCE_TECHNIQUE}\")\n",
        "print(f\"   TÃ©cnica balanceo DL: {DL_BALANCE_TECHNIQUE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0V_2v3X-Ap6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“ SECCIÃ“N 4: CREAR DIRECTORIOS\n",
        "# ============================================================================\n",
        "\n",
        "def create_run_dirs(cfg, tag=None):\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_id = f\"{ts}\" + (f\"_{tag}\" if tag else \"\")\n",
        "    cfg.RUN_ID = run_id\n",
        "    cfg.RUN_DIR = cfg.SAVE_DIR / run_id\n",
        "    cfg.PLOTS_DIR = cfg.RUN_DIR / \"plots\"\n",
        "    cfg.ARTIFACTS_DIR = cfg.RUN_DIR / \"artifacts\"\n",
        "    for d in (cfg.SAVE_DIR, cfg.RUN_DIR, cfg.PLOTS_DIR, cfg.ARTIFACTS_DIR):\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "    return cfg\n",
        "\n",
        "tag = f\"ML-{ML_BALANCE_TECHNIQUE}_DL-{DL_BALANCE_TECHNIQUE}\"\n",
        "create_run_dirs(cfg, tag=tag)\n",
        "print(f\"âœ… Directorios creados: {cfg.RUN_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZ1cBOo-DM0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”§ SECCIÃ“N 5: FUNCIONES DE FILTRADO\n",
        "# ============================================================================\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "def notch_filter(data, freq, fs, Q=30):\n",
        "    b, a = signal.iirnotch(freq, Q, fs)\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "def apply_filters(emg_data):\n",
        "    filtered = butter_bandpass_filter(emg_data, cfg.LOWCUT, cfg.HIGHCUT, cfg.FS)\n",
        "    filtered = notch_filter(filtered, cfg.NOTCH_FREQ, cfg.FS, cfg.NOTCH_Q)\n",
        "    return filtered\n",
        "\n",
        "print(\"âœ… Funciones de filtrado definidas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBsZ5nDX-GNh"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“‚ SECCIÃ“N 6: CARGA Y FILTRADO DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_filter_data():\n",
        "    \"\"\"\n",
        "    Carga y filtra seÃ±ales EMG segÃºn configuraciÃ³n de sujetos\n",
        "    \"\"\"\n",
        "    # Obtener todos los archivos .mat\n",
        "    all_mat_files = sorted(cfg.TRAIN_DIR.glob('S*.mat'))\n",
        "\n",
        "    # ðŸ”¥ FILTRAR SUJETOS SEGÃšN CONFIGURACIÃ“N\n",
        "    if USE_ALL_SUBJECTS:\n",
        "        mat_files = all_mat_files\n",
        "        print(f\"\\nðŸ“‚ Cargando TODOS los sujetos disponibles...\")\n",
        "    else:\n",
        "        mat_files = []\n",
        "        for subj_id in SELECTED_SUBJECTS:\n",
        "            file_path = cfg.TRAIN_DIR / f'S{subj_id}_E1_A1.mat'\n",
        "            if file_path.exists():\n",
        "                mat_files.append(file_path)\n",
        "            else:\n",
        "                print(f\"âš ï¸  Advertencia: S{subj_id} no encontrado\")\n",
        "\n",
        "        mat_files = sorted(mat_files)\n",
        "        print(f\"\\nðŸ“‚ Cargando {len(mat_files)} sujetos seleccionados: {SELECTED_SUBJECTS}\")\n",
        "\n",
        "    if len(mat_files) == 0:\n",
        "        raise ValueError(\"âŒ No se encontraron archivos de sujetos\")\n",
        "\n",
        "    print(f\"   Total de archivos a procesar: {len(mat_files)}\")\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    for i, mat_file in enumerate(mat_files, 1):\n",
        "        subj_id = mat_file.stem.split('_')[0]  # Extrae 'S1', 'S2', etc.\n",
        "\n",
        "        # Cargar archivo .mat\n",
        "        mat_data = loadmat(mat_file)\n",
        "        emg_raw = mat_data['emg'].astype(np.float32)\n",
        "\n",
        "        print(f\"   [{i}/{len(mat_files)}] {subj_id}: {emg_raw.shape}\")\n",
        "\n",
        "        # Aplicar filtros\n",
        "        emg_filtered = apply_filters(emg_raw)\n",
        "\n",
        "        # Guardar en diccionario\n",
        "        data_dict[subj_id] = {\n",
        "            'emg': emg_filtered,\n",
        "            'restimulus': mat_data['restimulus'].flatten(),\n",
        "            'rerepetition': mat_data['rerepetition'].flatten()\n",
        "        }\n",
        "\n",
        "        # Liberar memoria\n",
        "        del emg_raw, mat_data\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"âœ… {len(data_dict)} sujetos cargados y filtrados\")\n",
        "    return data_dict\n",
        "\n",
        "data_dict = load_and_filter_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZegBfVD-SMM"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”€ SECCIÃ“N 7: DIVISIÃ“N POR REPETICIONES\n",
        "# ============================================================================\n",
        "\n",
        "def split_by_repetitions(data_dict):\n",
        "    train_data, val_data, test_data = [], [], []\n",
        "\n",
        "    for subj_id, data in data_dict.items():\n",
        "        emg = data['emg']\n",
        "        stimulus = data['restimulus']\n",
        "        repetition = data['rerepetition']\n",
        "\n",
        "        mask_train = np.isin(repetition, cfg.TRAIN_REPS)\n",
        "        train_data.append({\n",
        "            'emg': emg[mask_train],\n",
        "            'stimulus': stimulus[mask_train],\n",
        "            'repetition': repetition[mask_train],\n",
        "            'subject': subj_id\n",
        "        })\n",
        "\n",
        "        mask_val = np.isin(repetition, cfg.VAL_REPS)\n",
        "        val_data.append({\n",
        "            'emg': emg[mask_val],\n",
        "            'stimulus': stimulus[mask_val],\n",
        "            'repetition': repetition[mask_val],\n",
        "            'subject': subj_id\n",
        "        })\n",
        "\n",
        "        mask_test = np.isin(repetition, cfg.TEST_REPS)\n",
        "        test_data.append({\n",
        "            'emg': emg[mask_test],\n",
        "            'stimulus': stimulus[mask_test],\n",
        "            'repetition': repetition[mask_test],\n",
        "            'subject': subj_id\n",
        "        })\n",
        "\n",
        "    def concatenate_data(data_list):\n",
        "        return {\n",
        "            'emg': np.vstack([d['emg'] for d in data_list]),\n",
        "            'stimulus': np.hstack([d['stimulus'] for d in data_list]),\n",
        "            'repetition': np.hstack([d['repetition'] for d in data_list])\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'train': concatenate_data(train_data),\n",
        "        'val': concatenate_data(val_data),\n",
        "        'test': concatenate_data(test_data)\n",
        "    }\n",
        "\n",
        "split_data = split_by_repetitions(data_dict)\n",
        "del data_dict\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nðŸ“Š Datos divididos:\")\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"   {split}: {split_data[split]['emg'].shape[0]:,} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV06zuqe-U2o"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”§ SECCIÃ“N 8: NORMALIZACIÃ“N Z-SCORE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nðŸ”§ Normalizando seÃ±ales...\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_emg_scaled = scaler.fit_transform(split_data['train']['emg'])\n",
        "val_emg_scaled = scaler.transform(split_data['val']['emg'])\n",
        "test_emg_scaled = scaler.transform(split_data['test']['emg'])\n",
        "\n",
        "with open(cfg.ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "split_data['train']['emg'] = train_emg_scaled.astype(np.float32)\n",
        "split_data['val']['emg'] = val_emg_scaled.astype(np.float32)\n",
        "split_data['test']['emg'] = test_emg_scaled.astype(np.float32)\n",
        "\n",
        "del train_emg_scaled, val_emg_scaled, test_emg_scaled\n",
        "gc.collect()\n",
        "print(\"âœ… NormalizaciÃ³n completada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWQptH46-XjT"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”ª SECCIÃ“N 9: VENTANAS Y ETIQUETADO\n",
        "# ============================================================================\n",
        "\n",
        "def create_windows(emg, stimulus, window_size, step_size):\n",
        "    n_samples = emg.shape[0]\n",
        "    windows, labels = [], []\n",
        "\n",
        "    for start in range(0, n_samples - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        window = emg[start:end]\n",
        "        window_labels = stimulus[start:end]\n",
        "\n",
        "        unique_labels, counts = np.unique(window_labels, return_counts=True)\n",
        "        if 0 in unique_labels:\n",
        "            continue\n",
        "\n",
        "        majority_label = unique_labels[np.argmax(counts)]\n",
        "        windows.append(window)\n",
        "        labels.append(majority_label)\n",
        "\n",
        "    return np.array(windows, dtype=np.float32), np.array(labels)\n",
        "\n",
        "def binarize_labels(labels, risk_movements):\n",
        "    return np.isin(labels, risk_movements).astype(np.int32)\n",
        "\n",
        "print(\"\\nðŸ”ª Creando ventanas...\")\n",
        "windowed_data = {}\n",
        "\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    emg = split_data[split_name]['emg']\n",
        "    stimulus = split_data[split_name]['stimulus']\n",
        "\n",
        "    windows, labels = create_windows(emg, stimulus, cfg.WINDOW_SIZE, cfg.STEP_SIZE)\n",
        "    binary_labels = binarize_labels(labels, cfg.RISK_MOVEMENTS)\n",
        "\n",
        "    windowed_data[split_name] = {\n",
        "        'windows': windows,\n",
        "        'labels': binary_labels,\n",
        "        'original_labels': labels\n",
        "    }\n",
        "\n",
        "    risk_count = (binary_labels == 1).sum()\n",
        "    safe_count = (binary_labels == 0).sum()\n",
        "    print(f\"   {split_name.upper()}: {len(windows):,} ventanas | Risk: {risk_count:,} | Safe: {safe_count:,}\")\n",
        "\n",
        "del split_data\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOYtkUBg-0Tp"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# âš™ï¸ SECCIÃ“N 10: EXTRACCIÃ“N DE FEATURES PARA ML\n",
        "# ============================================================================\n",
        "\n",
        "if NEED_FEATURES:\n",
        "    def extract_time_domain_features(window):\n",
        "        features = []\n",
        "        for ch in range(window.shape[1]):\n",
        "            sig = window[:, ch]\n",
        "            mav = np.mean(np.abs(sig))\n",
        "            wl = np.sum(np.abs(np.diff(sig)))\n",
        "            zc = np.sum(np.diff(np.sign(sig)) != 0)\n",
        "            diff_sig = np.diff(sig)\n",
        "            ssc = np.sum(np.diff(np.sign(diff_sig)) != 0)\n",
        "            rms = np.sqrt(np.mean(sig**2))\n",
        "            features.extend([mav, wl, zc, ssc, rms])\n",
        "        return np.array(features)\n",
        "\n",
        "    def extract_frequency_features(window, fs=2000):\n",
        "        features = []\n",
        "        for ch in range(window.shape[1]):\n",
        "            sig = window[:, ch]\n",
        "            freqs = np.fft.rfftfreq(len(sig), 1/fs)\n",
        "            psd = np.abs(np.fft.rfft(sig))**2\n",
        "            total_power = np.sum(psd)\n",
        "            if total_power > 0:\n",
        "                mean_freq = np.sum(freqs * psd) / total_power\n",
        "                cumsum = np.cumsum(psd)\n",
        "                median_freq = freqs[np.searchsorted(cumsum, total_power/2)]\n",
        "            else:\n",
        "                mean_freq = median_freq = 0\n",
        "            features.extend([mean_freq, median_freq])\n",
        "        return np.array(features)\n",
        "\n",
        "    def extract_wavelet_features(window, wavelet='db4', level=4):\n",
        "        \"\"\"Features wavelet (energÃ­a por nivel)\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for ch in range(window.shape[1]):\n",
        "            signal = window[:, ch]\n",
        "\n",
        "            # DWT\n",
        "            coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
        "\n",
        "            # EnergÃ­a de cada nivel\n",
        "            for coeff in coeffs:\n",
        "                energy = np.sum(coeff**2)\n",
        "                features.append(energy)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def extract_all_features(window):\n",
        "        time_feats = extract_time_domain_features(window)\n",
        "        freq_feats = extract_frequency_features(window)\n",
        "        wave_feats = extract_wavelet_features(window)\n",
        "        return np.concatenate([time_feats, freq_feats, wave_feats])\n",
        "\n",
        "    print(\"\\nâš™ï¸ Extrayendo features para ML...\")\n",
        "    features_data = {}\n",
        "\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        windows = windowed_data[split_name]['windows']\n",
        "        labels = windowed_data[split_name]['labels']\n",
        "\n",
        "        print(f\"   {split_name.upper()}: {len(windows):,} ventanas...\")\n",
        "        features_list = []\n",
        "\n",
        "        for i, window in enumerate(windows):\n",
        "            features = extract_all_features(window)\n",
        "            features_list.append(features)\n",
        "            if (i+1) % 10000 == 0:\n",
        "                print(f\"      {i+1:,}/{len(windows):,}\")\n",
        "\n",
        "        features_array = np.array(features_list, dtype=np.float32)\n",
        "        features_data[split_name] = {\n",
        "            'features': features_array,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "        print(f\"   âœ… Features shape: {features_array.shape}\")\n",
        "\n",
        "    del features_list\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b73enbXo_N-H"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”„ SECCIÃ“N 11: BALANCEO DE CLASES PARA ML\n",
        "# ============================================================================\n",
        "\n",
        "if NEED_FEATURES:\n",
        "    X_train = features_data['train']['features']\n",
        "    y_train = features_data['train']['labels']\n",
        "\n",
        "    original_safe = (y_train == 0).sum()\n",
        "    original_risk = (y_train == 1).sum()\n",
        "\n",
        "    if ML_BALANCE_TECHNIQUE == 'none':\n",
        "        print(\"\\nðŸ”„ ML: Sin balanceo (none)\")\n",
        "        print(f\"   Safe={original_safe:,} | Risk={original_risk:,}\")\n",
        "        # No hacemos nada, usamos los datos originales\n",
        "        balanced_safe = original_safe\n",
        "        balanced_risk = original_risk\n",
        "\n",
        "    elif ML_BALANCE_TECHNIQUE == 'adasyn':\n",
        "        print(\"\\nðŸ”„ ML: Aplicando ADASYN...\")\n",
        "        print(f\"   Antes: Safe={original_safe:,} | Risk={original_risk:,}\")\n",
        "        adasyn = ADASYN(random_state=cfg.RANDOM_STATE, n_neighbors=5)\n",
        "        X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
        "        balanced_safe = (y_train == 0).sum()\n",
        "        balanced_risk = (y_train == 1).sum()\n",
        "        print(f\"   DespuÃ©s: Safe={balanced_safe:,} | Risk={balanced_risk:,}\")\n",
        "\n",
        "    elif ML_BALANCE_TECHNIQUE == 'smote':\n",
        "        print(\"\\nðŸ”„ ML: Aplicando SMOTE...\")\n",
        "        print(f\"   Antes: Safe={original_safe:,} | Risk={original_risk:,}\")\n",
        "        smote = SMOTE(random_state=cfg.RANDOM_STATE, k_neighbors=5)\n",
        "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "        balanced_safe = (y_train == 0).sum()\n",
        "        balanced_risk = (y_train == 1).sum()\n",
        "        print(f\"   DespuÃ©s: Safe={balanced_safe:,} | Risk={balanced_risk:,}\")\n",
        "\n",
        "    # Actualizar features_data con los datos balanceados (o sin balancear)\n",
        "    features_data['train']['features'] = X_train.astype(np.float32)\n",
        "    features_data['train']['labels'] = y_train\n",
        "\n",
        "    del X_train, y_train\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1W3C01GowQM"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“Š SECCIÃ“N 11.5: VISUALIZACIÃ“N DISTRIBUCIÃ“N BINARIA ANTES/DESPUÃ‰S\n",
        "# ============================================================================\n",
        "\n",
        "if NEED_FEATURES and ML_BALANCE_TECHNIQUE != 'none':\n",
        "    print(\"\\nðŸ“Š Creando grÃ¡fica de distribuciÃ³n...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    categories = ['Antes del balanceo', 'DespuÃ©s del balanceo']\n",
        "    safe_counts = [original_safe, balanced_safe]\n",
        "    risk_counts = [original_risk, balanced_risk]\n",
        "\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, safe_counts, width, label='Safe', color='#2ecc71', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, risk_counts, width, label='Risk', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "    ax.set_ylabel('Cantidad de muestras', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'DistribuciÃ³n de Clases - ML ({ML_BALANCE_TECHNIQUE.upper()})', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories)\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Agregar valores en las barras\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{int(height):,}',\n",
        "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cfg.PLOTS_DIR / 'class_distribution_balance.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"âœ… GrÃ¡fica guardada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyq2w_YB_Qcu"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸŽ¯ SECCIÃ“N 12: FOCAL LOSS PARA DL\n",
        "# ============================================================================\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.75):\n",
        "    \"\"\"\n",
        "    ðŸ”¥ Focal Loss para manejar desbalance de clases\n",
        "    gamma: factor de enfoque (mayor = mÃ¡s peso a ejemplos difÃ­ciles)\n",
        "    alpha: peso para clase positiva\n",
        "    \"\"\"\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
        "\n",
        "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        focal_weight = K.pow(1.0 - pt, gamma)\n",
        "\n",
        "        focal_loss_value = alpha * focal_weight * cross_entropy\n",
        "\n",
        "        return K.mean(focal_loss_value)\n",
        "\n",
        "    return loss\n",
        "\n",
        "print(\"âœ… Focal Loss definido\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42YV9HKQ_UxF"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ§  SECCIÃ“N 13: ARQUITECTURA CNN+LSTM CON ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "def build_cnn_lstm_attention(input_shape, use_focal_loss=True):\n",
        "    \"\"\"ðŸ”¥ VersiÃ³n REDUCIDA para evitar overfitting\"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Conv Block 1 - Reducido\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(cfg.L2_REG))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1D(128, 3, activation='relu', padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(cfg.DROPOUT)(x)\n",
        "\n",
        "    # Conv Block 2 - Reducido\n",
        "    x = layers.Conv1D(256, 3, activation='relu', padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1D(256, 3, activation='relu', padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(cfg.DROPOUT)(x)\n",
        "\n",
        "    # LSTM - REDUCIDAS\n",
        "    x = layers.LSTM(64, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.Dropout(cfg.DROPOUT)(x)\n",
        "    x = layers.LSTM(32, return_sequences=True,\n",
        "                    kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "\n",
        "    # Attention\n",
        "    attention = layers.Dense(1, activation='tanh')(x)\n",
        "    attention = layers.Flatten()(attention)\n",
        "    attention = layers.Activation('softmax')(attention)\n",
        "    attention = layers.RepeatVector(32)(attention)\n",
        "    attention = layers.Permute([2, 1])(attention)\n",
        "\n",
        "    x = layers.Multiply()([x, attention])\n",
        "    x = layers.Lambda(attention_sum, name='attention_sum')(x)  # âœ… BUENO\n",
        "\n",
        "    # Dense - MÃ¡s regularizaciÃ³n\n",
        "    x = layers.Dense(32, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    # Seleccionar loss function segÃºn configuraciÃ³n\n",
        "    loss_fn = focal_loss(gamma=2.5, alpha=0.75) if use_focal_loss else 'binary_crossentropy'\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\n",
        "        loss=loss_fn,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# ðŸ§  SECCIÃ“N 14: BiLSTM CON ATTENTION (ANTI-OVERFITTING)\n",
        "# ============================================================================\n",
        "\n",
        "def build_bilstm_attention(input_shape, use_focal_loss=True):\n",
        "    \"\"\"ðŸ”¥ VersiÃ³n SIMPLIFICADA para evitar overfitting\"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # BiLSTM - REDUCIDAS\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True,\n",
        "                                         kernel_regularizer=regularizers.l2(cfg.L2_REG)))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(cfg.DROPOUT)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True,\n",
        "                                         kernel_regularizer=regularizers.l2(cfg.L2_REG)))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Attention\n",
        "    attention = layers.Dense(1, activation='tanh')(x)\n",
        "    attention = layers.Flatten()(attention)\n",
        "    attention = layers.Activation('softmax')(attention)\n",
        "    attention = layers.RepeatVector(64)(attention)\n",
        "    attention = layers.Permute([2, 1])(attention)\n",
        "\n",
        "    x = layers.Multiply()([x, attention])\n",
        "    x = layers.Lambda(attention_sum, name='attention_sum')(x)  # âœ… BUENO\n",
        "\n",
        "    # Dense con mÃ¡s regularizaciÃ³n\n",
        "    x = layers.Dense(32, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(16, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(cfg.L2_REG))(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Seleccionar loss function segÃºn configuraciÃ³n\n",
        "    loss_fn = focal_loss(gamma=2.5, alpha=0.75) if use_focal_loss else 'binary_crossentropy'\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\n",
        "        loss=loss_fn,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"âœ… BiLSTM con Attention definido\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js7IHFsX_a2m"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”„ SECCIÃ“N 15: DATA AUGMENTATION PARA DL\n",
        "# ============================================================================\n",
        "\n",
        "def augment_signal(signal, augmentation_factor=0.7):\n",
        "    \"\"\"ðŸ”¥ Data Augmentation MÃS AGRESIVO\"\"\"\n",
        "    if np.random.rand() > augmentation_factor:\n",
        "        return signal\n",
        "\n",
        "    augmented = signal.copy()\n",
        "\n",
        "    # Jittering mÃ¡s fuerte\n",
        "    if np.random.rand() < 0.6:\n",
        "        noise = np.random.normal(0, 0.025, augmented.shape)\n",
        "        augmented = augmented + noise\n",
        "\n",
        "    # Scaling mÃ¡s agresivo\n",
        "    if np.random.rand() < 0.6:\n",
        "        scale_factor = np.random.uniform(0.85, 1.15)\n",
        "        augmented = augmented * scale_factor\n",
        "\n",
        "    # Time warping mÃ¡s agresivo\n",
        "    if np.random.rand() < 0.4:\n",
        "        warp_factor = np.random.uniform(0.92, 1.08)\n",
        "        new_length = int(len(augmented) * warp_factor)\n",
        "        if new_length != len(augmented) and new_length > 0:\n",
        "            indices = np.linspace(0, len(augmented)-1, new_length)\n",
        "            augmented_warped = np.zeros((new_length, augmented.shape[1]))\n",
        "            for ch in range(augmented.shape[1]):\n",
        "                augmented_warped[:, ch] = np.interp(indices, np.arange(len(augmented)), augmented[:, ch])\n",
        "            indices_back = np.linspace(0, new_length-1, len(augmented))\n",
        "            for ch in range(augmented.shape[1]):\n",
        "                augmented[:, ch] = np.interp(indices_back, np.arange(new_length), augmented_warped[:, ch])\n",
        "\n",
        "    return augmented.astype(np.float32)\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"ðŸ”¥ Generador con Data Augmentation\"\"\"\n",
        "    def __init__(self, X, y, batch_size=64, augment=False, shuffle=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.X))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch = self.X[indices]\n",
        "        y_batch = self.y[indices]\n",
        "\n",
        "        if self.augment:\n",
        "            X_batch = np.array([augment_signal(x) for x in X_batch])\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "print(\"âœ… Data Augmentation y Generador definidos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qjq8PC2_jFU"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”¬ SECCIÃ“N 16: ENTRENAMIENTO MODELOS ML\n",
        "# ============================================================================\n",
        "\n",
        "ml_results = None\n",
        "\n",
        "if len(ML_MODELS) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ”¬ ENTRENAMIENTO MODELOS ML\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train = features_data['train']['features']\n",
        "    y_train = features_data['train']['labels']\n",
        "    X_val = features_data['val']['features']\n",
        "    y_val = features_data['val']['labels']\n",
        "    X_test = features_data['test']['features']\n",
        "    y_test = features_data['test']['labels']\n",
        "\n",
        "    ml_results = {}\n",
        "\n",
        "    # Modelo 1: Ensemble Subspace KNN\n",
        "    if '1' in ML_MODELS:\n",
        "        print(\"\\nðŸ”¹ Entrenando Ensemble Subspace KNN...\")\n",
        "        base_knn = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='euclidean')\n",
        "        model = BaggingClassifier(\n",
        "            estimator=base_knn,\n",
        "            n_estimators=30,\n",
        "            max_samples=0.7,\n",
        "            max_features=0.8,\n",
        "            random_state=cfg.RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred_val = model.predict(X_val)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        ml_results['Ensemble_KNN'] = {\n",
        "            'model': model,\n",
        "            'val': {'y_true': y_val, 'y_pred': y_pred_val},\n",
        "            'test': {'y_true': y_test, 'y_pred': y_pred_test}\n",
        "        }\n",
        "        # ðŸ’¾ GUARDAR MODELO\n",
        "        with open(cfg.ARTIFACTS_DIR / 'ensemble_knn.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(\"   ðŸ’¾ Modelo guardado: ensemble_knn.pkl\")\n",
        "        print(\"   âœ… Completado\")\n",
        "\n",
        "    # Modelo 2: Random Forest\n",
        "    if '2' in ML_MODELS:\n",
        "        print(\"\\nðŸ”¹ Entrenando Random Forest...\")\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=30,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            random_state=cfg.RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred_val = model.predict(X_val)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        ml_results['RandomForest'] = {\n",
        "            'model': model,\n",
        "            'val': {'y_true': y_val, 'y_pred': y_pred_val},\n",
        "            'test': {'y_true': y_test, 'y_pred': y_pred_test}\n",
        "        }\n",
        "        # ðŸ’¾ GUARDAR MODELO\n",
        "        with open(cfg.ARTIFACTS_DIR / 'random_forest.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print(\"   ðŸ’¾ Modelo guardado: random_forest.pkl\")\n",
        "        print(\"   âœ… Completado\")\n",
        "\n",
        "# ============================================================================\n",
        "# ðŸ§  SECCIÃ“N 17: ENTRENAMIENTO MODELOS DL\n",
        "# ============================================================================\n",
        "\n",
        "dl_results = None\n",
        "\n",
        "if len(DL_MODELS) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ§  ENTRENAMIENTO MODELOS DL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train = windowed_data['train']['windows']\n",
        "    y_train = windowed_data['train']['labels'].reshape(-1, 1)\n",
        "    X_val = windowed_data['val']['windows']\n",
        "    y_val = windowed_data['val']['labels'].reshape(-1, 1)\n",
        "    X_test = windowed_data['test']['windows']\n",
        "    y_test = windowed_data['test']['labels'].reshape(-1, 1)\n",
        "\n",
        "    print(f\"\\nðŸ“Š Shapes DL:\")\n",
        "    print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "    print(f\"   X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "    # Determinar configuraciÃ³n segÃºn tÃ©cnica de balanceo DL\n",
        "    use_focal_loss = DL_BALANCE_TECHNIQUE in ['focal_loss', 'focal_loss+augment']\n",
        "    use_augmentation = DL_BALANCE_TECHNIQUE in ['augment_only', 'focal_loss+augment']\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ ConfiguraciÃ³n DL:\")\n",
        "    print(f\"   Loss: {'Focal Loss' if use_focal_loss else 'Binary Crossentropy'}\")\n",
        "    print(f\"   Augmentation: {'Activado' if use_augmentation else 'Desactivado'}\")\n",
        "\n",
        "    # Generadores\n",
        "    train_gen = DataGenerator(X_train, y_train, batch_size=cfg.BATCH_SIZE, augment=use_augmentation, shuffle=True)\n",
        "    val_gen = DataGenerator(X_val, y_val, batch_size=cfg.BATCH_SIZE, augment=False, shuffle=False)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        min_delta=0.001,\n",
        "        restore_best_weights=True,\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.3,\n",
        "        patience=8,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    dl_results = {}\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Modelo 3: CNN+LSTM con Attention\n",
        "    if '3' in DL_MODELS:\n",
        "        print(\"\\nðŸ”¹ Entrenando CNN+LSTM con Attention...\")\n",
        "        model = build_cnn_lstm_attention(input_shape, use_focal_loss=use_focal_loss)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=cfg.EPOCHS,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Predicciones\n",
        "        y_pred_val = (model.predict(X_val, batch_size=cfg.BATCH_SIZE) > 0.5).astype(int)\n",
        "        y_pred_test = (model.predict(X_test, batch_size=cfg.BATCH_SIZE) > 0.5).astype(int)\n",
        "\n",
        "        dl_results['CNN_LSTM_Attention'] = {\n",
        "            'model': model,\n",
        "            'history': history.history,\n",
        "            'val': {'y_true': y_val.flatten(), 'y_pred': y_pred_val.flatten()},\n",
        "            'test': {'y_true': y_test.flatten(), 'y_pred': y_pred_test.flatten()}\n",
        "        }\n",
        "\n",
        "        # Guardar modelo\n",
        "        #model.save(cfg.ARTIFACTS_DIR / 'cnn_lstm_attention.h5')\n",
        "        model.save(cfg.ARTIFACTS_DIR / 'cnn_lstm_attention.keras')\n",
        "        print(\"   âœ… Completado\")\n",
        "\n",
        "    # Modelo 4: BiLSTM con Attention\n",
        "    if '4' in DL_MODELS:\n",
        "        print(\"\\nðŸ”¹ Entrenando BiLSTM con Attention...\")\n",
        "        model = build_bilstm_attention(input_shape, use_focal_loss=use_focal_loss)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=cfg.EPOCHS,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Predicciones\n",
        "        y_pred_val = (model.predict(X_val, batch_size=cfg.BATCH_SIZE) > 0.5).astype(int)\n",
        "        y_pred_test = (model.predict(X_test, batch_size=cfg.BATCH_SIZE) > 0.5).astype(int)\n",
        "\n",
        "        dl_results['BiLSTM_Attention'] = {\n",
        "            'model': model,\n",
        "            'history': history.history,\n",
        "            'val': {'y_true': y_val.flatten(), 'y_pred': y_pred_val.flatten()},\n",
        "            'test': {'y_true': y_test.flatten(), 'y_pred': y_pred_test.flatten()}\n",
        "        }\n",
        "\n",
        "        # Guardar modelo\n",
        "        #model.save(cfg.ARTIFACTS_DIR / 'bilstm_attention.h5')\n",
        "        model.save(cfg.ARTIFACTS_DIR / 'bilstm_attention.keras')\n",
        "        print(\"   âœ… Completado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu_xkhKs_mGA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸŽ¯ SECCIÃ“N 18: THRESHOLD OPTIMIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ¯ OPTIMIZACIÃ“N DE THRESHOLD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def optimize_threshold(y_true, y_proba):\n",
        "    \"\"\"ðŸ”¥ Encuentra el threshold Ã³ptimo que maximiza F1\"\"\"\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "\n",
        "    for threshold in np.arange(0.3, 0.8, 0.05):\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "# Optimizar para cada modelo\n",
        "optimized_thresholds = {}\n",
        "\n",
        "if dl_results is not None:\n",
        "    for model_name, results in dl_results.items():\n",
        "        print(f\"\\nðŸ” Optimizando threshold para {model_name}...\")\n",
        "\n",
        "        # Obtener probabilidades en validation set\n",
        "        model = results['model']\n",
        "        y_val_true = windowed_data['val']['labels']\n",
        "        y_val_proba = model.predict(windowed_data['val']['windows'], batch_size=cfg.BATCH_SIZE).flatten()\n",
        "\n",
        "        best_thresh, best_f1_val = optimize_threshold(y_val_true, y_val_proba)\n",
        "        optimized_thresholds[model_name] = best_thresh\n",
        "\n",
        "        print(f\"   Threshold Ã³ptimo: {best_thresh:.2f}\")\n",
        "        print(f\"   F1 en validation: {best_f1_val:.4f}\")\n",
        "\n",
        "        # Re-predecir en test con threshold Ã³ptimo\n",
        "        y_test_proba = model.predict(windowed_data['test']['windows'], batch_size=cfg.BATCH_SIZE).flatten()\n",
        "        y_test_pred_optimized = (y_test_proba >= best_thresh).astype(int)\n",
        "\n",
        "        # Actualizar resultados\n",
        "        dl_results[model_name]['test']['y_pred'] = y_test_pred_optimized\n",
        "        dl_results[model_name]['optimized_threshold'] = best_thresh\n",
        "\n",
        "print(\"\\nâœ… Threshold optimization completada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qexj4YTa_rY9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“Š SECCIÃ“N 19: EVALUACIÃ“N FINAL\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name, split='test'):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{model_name} - {split.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š EVALUACIÃ“N FINAL EN TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_metrics = {}\n",
        "\n",
        "if ml_results is not None:\n",
        "    for model_name, results in ml_results.items():\n",
        "        metrics = evaluate_model(\n",
        "            results['test']['y_true'],\n",
        "            results['test']['y_pred'],\n",
        "            model_name,\n",
        "            'test'\n",
        "        )\n",
        "        all_metrics[model_name] = metrics\n",
        "\n",
        "if dl_results is not None:\n",
        "    for model_name, results in dl_results.items():\n",
        "        metrics = evaluate_model(\n",
        "            results['test']['y_true'],\n",
        "            results['test']['y_pred'],\n",
        "            model_name,\n",
        "            'test'\n",
        "        )\n",
        "        all_metrics[model_name] = metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImX01n4Z9cUC"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ“ˆ SECCIÃ“N 20: VISUALIZACIÃ“N DE RESULTADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ComparaciÃ³n de mÃ©tricas\n",
        "def plot_metrics_comparison(metrics_dict):\n",
        "    df = pd.DataFrame(metrics_dict).T\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    df.plot(kind='bar', ax=ax, width=0.7, colormap='viridis')\n",
        "\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Modelo', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('ComparaciÃ³n de MÃ©tricas en Test Set', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.legend(title='MÃ©trica', fontsize=10)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=8)\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cfg.PLOTS_DIR / 'metrics_comparison.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics_comparison(all_metrics)\n",
        "\n",
        "# Matrices de confusiÃ³n\n",
        "def plot_confusion_matrices(ml_results, dl_results):\n",
        "    all_results = {}\n",
        "    if ml_results:\n",
        "        all_results.update(ml_results)\n",
        "    if dl_results:\n",
        "        all_results.update(dl_results)\n",
        "\n",
        "    n_models = len(all_results)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
        "\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (model_name, results) in enumerate(all_results.items()):\n",
        "        y_true = results['test']['y_true']\n",
        "        y_pred = results['test']['y_pred']\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='.2%', cmap='Blues',\n",
        "                    xticklabels=['Safe', 'Risk'],\n",
        "                    yticklabels=['Safe', 'Risk'],\n",
        "                    ax=axes[i], cbar_kws={'label': 'ProporciÃ³n'})\n",
        "\n",
        "        axes[i].set_title(f'{model_name}', fontweight='bold')\n",
        "        axes[i].set_ylabel('True Label')\n",
        "        axes[i].set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cfg.PLOTS_DIR / 'confusion_matrices.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrices(ml_results, dl_results)\n",
        "\n",
        "# Curvas ROC\n",
        "def plot_roc_curves(ml_results, dl_results):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    all_results = {}\n",
        "    if ml_results:\n",
        "        all_results.update(ml_results)\n",
        "    if dl_results:\n",
        "        all_results.update(dl_results)\n",
        "\n",
        "    for model_name, results in all_results.items():\n",
        "        y_true = results['test']['y_true']\n",
        "        y_pred = results['test']['y_pred']\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "        auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC={auc:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.5)', linewidth=2)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.title('Curvas ROC - Test Set', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cfg.PLOTS_DIR / 'roc_curves.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curves(ml_results, dl_results)\n",
        "\n",
        "# Training history (DL)\n",
        "if dl_results is not None:\n",
        "    n_models = len(dl_results)\n",
        "    fig, axes = plt.subplots(n_models, 2, figsize=(14, 4*n_models))\n",
        "\n",
        "    if n_models == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i, (model_name, results) in enumerate(dl_results.items()):\n",
        "        history = results['history']\n",
        "\n",
        "        # Loss\n",
        "        axes[i, 0].plot(history['loss'], label='Train Loss', linewidth=2)\n",
        "        axes[i, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "        axes[i, 0].set_title(f'{model_name} - Loss', fontweight='bold')\n",
        "        axes[i, 0].set_xlabel('Epoch')\n",
        "        axes[i, 0].set_ylabel('Loss')\n",
        "        axes[i, 0].legend()\n",
        "        axes[i, 0].grid(alpha=0.3)\n",
        "\n",
        "        # Accuracy\n",
        "        axes[i, 1].plot(history['accuracy'], label='Train Acc', linewidth=2)\n",
        "        axes[i, 1].plot(history['val_accuracy'], label='Val Acc', linewidth=2)\n",
        "        axes[i, 1].set_title(f'{model_name} - Accuracy', fontweight='bold')\n",
        "        axes[i, 1].set_xlabel('Epoch')\n",
        "        axes[i, 1].set_ylabel('Accuracy')\n",
        "        axes[i, 1].legend()\n",
        "        axes[i, 1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cfg.PLOTS_DIR / 'training_history.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrE_hNB7_wUr"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ’¾ SECCIÃ“N 21: GUARDAR RESULTADOS\n",
        "# ============================================================================\n",
        "\n",
        "# Preparar descripciÃ³n de tÃ©cnicas para metadata\n",
        "ml_technique_desc = {\n",
        "    'none': 'Sin balanceo',\n",
        "    'adasyn': 'ADASYN',\n",
        "    'smote': 'SMOTE'\n",
        "}[ML_BALANCE_TECHNIQUE]\n",
        "\n",
        "dl_technique_desc = {\n",
        "    'none': 'Binary Crossentropy',\n",
        "    'augment_only': 'Binary Crossentropy + Data Augmentation',\n",
        "    'focal_loss': 'Focal Loss (gamma=2.5, alpha=0.75)',\n",
        "    'focal_loss+augment': 'Focal Loss (gamma=2.5, alpha=0.75) + Data Augmentation'\n",
        "}[DL_BALANCE_TECHNIQUE]\n",
        "\n",
        "metadata = {\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'config': {\n",
        "        'window_size_ms': cfg.WINDOW_SIZE_MS,\n",
        "        'overlap': cfg.OVERLAP,\n",
        "        'batch_size': cfg.BATCH_SIZE,\n",
        "        'dropout': cfg.DROPOUT,\n",
        "        'learning_rate': cfg.LEARNING_RATE,\n",
        "        'ml_balance_technique': ML_BALANCE_TECHNIQUE,\n",
        "        'ml_balance_description': ml_technique_desc,\n",
        "        'dl_balance_technique': DL_BALANCE_TECHNIQUE,\n",
        "        'dl_balance_description': dl_technique_desc\n",
        "    },\n",
        "    'optimized_thresholds': optimized_thresholds,\n",
        "    'results': all_metrics\n",
        "}\n",
        "\n",
        "with open(cfg.ARTIFACTS_DIR / 'metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"\\nâœ… Metadata guardada\")\n",
        "\n",
        "# Tabla resumen\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“‹ RESUMEN FINAL\")\n",
        "print(\"=\"*70)\n",
        "df_results = pd.DataFrame(all_metrics).T\n",
        "df_results = df_results.round(4)\n",
        "print(df_results)\n",
        "df_results.to_csv(cfg.ARTIFACTS_DIR / 'results_summary.csv')\n",
        "\n",
        "print(f\"\\nðŸŽ‰ ENTRENAMIENTO COMPLETADO\")\n",
        "print(f\"ðŸ“ Resultados guardados en: {cfg.RUN_DIR}\")\n",
        "print(f\"ðŸ“Š GrÃ¡ficas en: {cfg.PLOTS_DIR}\")\n",
        "print(f\"ðŸ’¾ Artefactos en: {cfg.ARTIFACTS_DIR}\")\n",
        "print(f\"\\nâš™ï¸ TÃ©cnicas utilizadas:\")\n",
        "print(f\"   ML: {ml_technique_desc}\")\n",
        "print(f\"   DL: {dl_technique_desc}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
